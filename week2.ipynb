{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5142084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a244a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07339914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c98b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79fa3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A challenge to test the competitive spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f06a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf953c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "0.5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f0fd5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a spicy challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0ef8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" — if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" — if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7652438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll choose... Share.\n",
       "\n",
       "Since I'm playing the game with a partner, we're essentially in this together (or rather, not-so-together). We each gain something from cooperating: if both of us choose \"Share\", we get $1,000 each. On the other hand, defecting may result in either one of us getting nothing or $2,000.\n",
       "\n",
       "I'd like to assume that my partner will also make the same choice (i.e., they'll choose \"Share\"). Not only does sharing provide a mutual benefit for both of us, but it also allows us to maintain some trust and cooperation with each other. Who knows what our individual interests are? If we're honest about our goals and intentions, we might actually work out better together.\n",
       "\n",
       "Let's take the risk and choose to share!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8037a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another hard puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84322ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d14ce16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To find the total distance the worm gnawed through, we need to calculate the thickness of the volumes and add it to the thickness of each cover.\n",
       "\n",
       "Each volume has a thickness of 2 cm. Since there are two volumes, we multiply this value by 2:\n",
       "\n",
       "2 cm/volume × 2 volumes = 4 cm\n",
       "\n",
       "Now we add the thickness of both covers (each 2 mm thick):\n",
       "\n",
       "4 cm + (2 mm + 2 mm) \n",
       "= 4cm+4mm \n",
       "\n",
       "The answer: 10/3= 4cm and 4,mm."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af306b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8086b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02fdd3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's one:\n",
       "\n",
       "What do you call a fake noodle?\n",
       "\n",
       "An impasta!\n",
       "\n",
       "Hope that made you smile! Do you want to hear another?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "response = llm.invoke(\"tell_a_joke\")\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d6bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98c11a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the neural network go to therapy?\n",
       "\n",
       "Because it was struggling with its weights and biases!\n",
       "\n",
       "As a student on your journey to becoming an expert in LLM Engineering, remember that just like a well-calibrated model, your understanding of language and cognition should be nuanced and balanced. Keep working through the layers, and don't be afraid to retrain your perspective!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"ollama/llama3.2\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3be4181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 46\n",
      "Output tokens: 74\n",
      "Total tokens: 120\n",
      "Total cost: 0.0000 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22d51eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1a01020",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6615bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In Hamlet, when Laertes asks \"Where is my father?\" his reply comes from Ophelia, who had been searching for her father Polonius's body in the castle. She tells Laertes that she hasn't found it yet.\n",
       "\n",
       "The quote is: \n",
       "\n",
       "\"Tell me, where he lies.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"ollama/llama3.2\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
