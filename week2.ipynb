{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5142084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a244a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07339914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13c98b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "from openai import OpenAI\n",
    "openai = OpenAI()\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79fa3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A challenge to test the competitive spirit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f06a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf953c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "0.5"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f0fd5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a spicy challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0ef8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "dilemma_prompt = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "Cooperate: Choose \"Share\" — if both of you choose this, you each win $1,000.\n",
    "Defect: Choose \"Steal\" — if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "If both steal, you both get nothing.\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\"\n",
    "\n",
    "dilemma = [\n",
    "    {\"role\": \"user\", \"content\": dilemma_prompt},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7652438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I will choose... Share. This seems like a relatively safe choice since it guarantees at least $1,000 for me if my partner also chooses Share. By cooperating, I'm hoping that we can reach an agreement with our partner and maximize our combined winnings. Plus, the punishment of getting nothing if both steal doesn't seem too likely given the large rewards at stake."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=dilemma)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb8037a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another hard puzzle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84322ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "hard_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d14ce16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break this problem down step by step.\n",
       "\n",
       "The total thickness of a book is the sum of the thickness of cover, pages, and another cover. \n",
       "\n",
       "If we assume x as the number of pages in both books then \n",
       " Total thickness= 2 * (mm) + x (cm)x (mm)\n",
       "\n",
       "The worm gnawed from the first page of the first volume to the last page of the second volume. That means it gnawed through x pages. Since each pages is of 1 cm thickness, it will gnaw 2x cm.\n",
       "\n",
       "Now let's equate two values:\n",
       "\n",
       " \n",
       "We know that \n",
       "\n",
       "2*(mm) + 100 (cm)x(mm)= 200-x mm\n",
       "2*(cm)x * (mm)+ 100= 2(100- x)\n",
       "2(100)(mm)x+  100 =200 mm*x -X (mm)\n",
       "\n",
       "Simplify\n",
       "\n",
       "600x + 100 =  200x\n",
       "400= - 200x  \n",
       "x=   400* -1/200      \n",
       "x=- 2\n",
       "\n",
       "\n",
       "\n",
       "That means we have taken the wrong assumption. \n",
       "\n",
       "We can assume that worm gnaws from page one of a to xth page and goes till z pages and now subtracting both, we get: \n",
       " \n",
       "\n",
       " \n",
       "\n",
       "2cm(x pages)\n",
       "Plus two cover = ( 2 mm) + (10 cm)\n",
       "\n",
       "\n",
       "And now lets say, after some experimentation ,we arrived with x=14.\n",
       "\n",
       "Thus, the value for worm is as follows:\n",
       "\n",
       "\n",
       "\n",
       " \n",
       "x : 10 cm - 6 *1 cm + 12/4 \n",
       "2(18).(mm)* (13cm)\n",
       "So the final distance is: 312- 30 mm"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=hard_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5af306b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8086b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02fdd3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rujjul Saha\\AppData\\Local\\Temp\\ipykernel_28644\\1997861182.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2\")\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's one:\n",
       "\n",
       "What do you call a fake noodle?\n",
       "\n",
       "An impasta!\n",
       "\n",
       "(Sorry, I know it's a bit of a groaner, but I hope it brought a smile to your face!)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "response = llm.invoke(\"tell_a_joke\")\n",
    "display(Markdown(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02d6bf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LiteLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98c11a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM model go to therapy?\n",
       "\n",
       "Because it was struggling to \"transform\" its thoughts into coherent sentences, and was feeling a little \" masked\" by its complexity! But don't worry, with some fine-tuning and a solid understanding of its limitations, it's sure to \"fine-tune\" its skills and become an expert in no time!\n",
       "\n",
       "(Disclaimer: As a joke, this is not meant to be taken literally. LLMs are complex systems that require careful design, training, and testing to achieve their full potential. But I hope it brings a smile to your face!)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"ollama/llama3.2\", messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3be4181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 46\n",
      "Output tokens: 122\n",
      "Total tokens: 168\n",
      "Total cost: 0.0000 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb8b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LiteLLM to illustrate a Pro-feature: Prompt Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22d51eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak, man.\n",
      "  Laer. Where is my father?\n",
      "  King. Dead.\n",
      "  Queen. But not by him!\n",
      "  King. Let him deman\n"
     ]
    }
   ],
   "source": [
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    hamlet = f.read()\n",
    "\n",
    "loc = hamlet.find(\"Speak, man\")\n",
    "print(hamlet[loc:loc+100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1a01020",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6615bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When Laertes asks \"O, where is my father?\" in Act 1, Scene 1 of Hamlet, the reply from Polonius (who is pretending to be Laertes) is:\n",
       "\n",
       "\"Sir, I am your son.\"\n",
       "\n",
       "However, it's worth noting that this response is actually a ruse by Polonius to test whether Laertes will recognize his voice and return to Elsinore."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"ollama/llama3.2\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4de97ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 48\n",
      "Output tokens: 87\n",
      "Total tokens: 135\n",
      "Total cost: 0.0000 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a5add69",
   "metadata": {},
   "outputs": [],
   "source": [
    "question[0][\"content\"] += \"\\n\\nFor context, here is the entire text of Hamlet:\\n\\n\"+hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9617668a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a summary and analysis of the final scene of William Shakespeare's \"Hamlet\", Act 5, Scene 2:\n",
       "\n",
       "The scene begins with Horatio, who has survived the tragic events that unfolded in Elsinore Castle, speaking to Fortinbras, the Prince of Norway. Horatio recounts the story of Hamlet's death, which he witnessed from afar.\n",
       "\n",
       "Fortinbras arrives on the scene and is shocked by the sight of Hamlet's body on stage. He asks Horatio to tell him what happened, and Horatio delivers a somber account of the events leading up to Hamlet's demise.\n",
       "\n",
       "As Horatio speaks, Fortinbras and his captains approach the bodies of Laertes and Claudius (the King). The scene builds towards the climax as the soldiers prepare to shoot down the bodies of the three kings, including Hamlet.\n",
       "\n",
       "The final image is one of chaos and destruction. The stage is littered with the bodies of the dead monarchs, and the sound of cannons and gunfire fills the air. Fortinbras's words \"What feast is toward in thine eternal cell / That thou so many princes at a shot / So bloodily hast struck\" (lines 24-26) suggest that he is both horrified by what has happened and impressed by the sheer scale of the destruction.\n",
       "\n",
       "The scene serves as a commentary on the cyclical nature of violence and the futility of human ambition. The three kings - Claudius, Laertes, and Hamlet - are all killed in a single, bloody confrontation, symbolizing the destructive power of unchecked desire and greed.\n",
       "\n",
       "In terms of dramatic technique, the final scene is notable for its use of imagery and symbolism. The bodies on stage serve as a physical representation of the destruction and chaos that has been unleashed, while Fortinbras's words and actions convey his sense of shock and dismay.\n",
       "\n",
       "The play ends with a sense of uncertainty and ambiguity, leaving the audience to ponder the implications of what they have just witnessed. As Horatio says at the end of the scene, \"So shall You hear / Of carnal, bloody and unnatural acts\" (lines 31-32), suggesting that this is only one chapter in a larger story of human suffering and conflict.\n",
       "\n",
       "Overall, the final scene of Hamlet is a powerful exploration of the darker aspects of human nature and the devastating consequences of unchecked ambition."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = completion(model=\"ollama/llama3.2\", messages=question)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9720cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 4096\n",
      "Output tokens: 481\n",
      "Cached tokens: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "# Get cached tokens safely\n",
    "prompt_details = getattr(response.usage, 'prompt_tokens_details', None)\n",
    "cached_tokens = getattr(prompt_details, 'cached_tokens', 0) if prompt_details else 0\n",
    "print(f\"Cached tokens: {cached_tokens}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
